{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import h5py\n",
    "import numpy as np\n",
    "from functions.preprocess import input_shaping, split_index\n",
    "from functions.decoders import lstm_decoder\n",
    "from functions.metrics import compute_rmse, compute_pearson\n",
    "import time as timer\n",
    "from tensorflow import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation\n"
     ]
    }
   ],
   "source": [
    "seed = 2020 # random seed for reproducibility\n",
    "\n",
    "print (\"Starting simulation\")\n",
    "run_start = timer.time()\n",
    "\n",
    "feature_list = ['sua_rate','mua_rate']\n",
    "feature = feature_list[1] # select which spike feature: SUA=0, MUA=1\n",
    "\n",
    "# specify filename to be processed (choose from the list available at https://zenodo.org/record/583331)\n",
    "file_name = 'indy_20160915_01'          # file name\n",
    "kinematic_folder = 'kinematic_data/'    # kinematic data folder\n",
    "feature_folder = 'spike_data/features/' # spike features folder\n",
    "result_folder = 'results/'              # results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdw_time = 0.256 # window size in second\n",
    "lag = -32 # lag between kinematic and feature data (minus indicate feature lagging behaind kinematic)\n",
    "delta_time = 0.004 # sampling interval in second\n",
    "wdw_samp = int(round(wdw_time/delta_time))\n",
    "ol_samp = wdw_samp-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input features from file: spike_data/features/indy_20160915_01_spike_features_256ms.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# open spike features from hdf5 file\n",
    "feature_file = feature_folder+file_name+'_spike_features_'+str(int(wdw_time*1e3))+'ms.h5'\n",
    "print (\"Loading input features from file: \"+feature_file)\n",
    "with h5py.File(feature_file,'r') as f:\n",
    "    input_feature = f[feature].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading kinematic data from file: kinematic_data/indy_20160915_01_kinematic_data.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# open kinematic data from hdf5 file\n",
    "kinematic_file = kinematic_folder+file_name+'_kinematic_data.h5'\n",
    "print (\"Loading kinematic data from file: \"+kinematic_file)\n",
    "#fields: ['cursor_acc', 'cursor_pos', 'cursor_time', 'cursor_vel', 'target_pos']\n",
    "with h5py.File(kinematic_file,'r') as f:\n",
    "    cursor_vel = f['cursor_vel'].value # in mm/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data to fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "\n",
    "X_ss = ss.fit_transform(input_feature)\n",
    "y_mm = mm.fit_transform(cursor_vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95262, 88) (95262, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_ss.shape, y_mm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 80,000 for training\n",
    "\n",
    "X_train = X_ss[:80000, :]\n",
    "X_test = X_ss[80000:, :]\n",
    "\n",
    "y_train = y_mm[:80000, :]\n",
    "y_test = y_mm[80000:, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape (80000, 88) (80000, 2)\n",
      "Testing Shape (15262, 88) (15262, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the data for pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #pytorch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting nparrays to tensors for pytorch\n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([80000, 1, 88]) torch.Size([80000, 2])\n",
      "Testing Shape torch.Size([15262, 1, 88]) torch.Size([15262, 2])\n"
     ]
    }
   ],
   "source": [
    "#reshaping to rows, timestamps, features\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n",
    "\n",
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "    super(LSTM1, self).__init__()\n",
    "    self.num_classes = num_classes #number of classes\n",
    "    self.num_layers = num_layers #number of layers\n",
    "    self.input_size = input_size #input size\n",
    "    self.hidden_size = hidden_size #hidden state\n",
    "    self.seq_length = seq_length #sequence length\n",
    " \n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                      num_layers=num_layers, batch_first=True) #lstm\n",
    "    self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "    self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "    self.relu = nn.ReLU() \n",
    "    \n",
    "  def forward(self,x):\n",
    "    h_0 = Variable(torch.zeros(\n",
    "          self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "     \n",
    "    c_0 = Variable(torch.zeros(\n",
    "        self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "   \n",
    "    # Propagate input through LSTM\n",
    "\n",
    "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "   \n",
    "    hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "\n",
    "    out = self.relu(hn)\n",
    "\n",
    "    out = self.fc_1(out) #Fully connected layer\n",
    "\n",
    "    out = self.relu(out) #relu\n",
    "\n",
    "    out = self.fc(out) #Final Output\n",
    "   \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 88 #number of features\n",
    "hidden_size = 32 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 2 #number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]) #our lstm class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.26248\n",
      "Epoch: 10, loss: 0.13260\n",
      "Epoch: 20, loss: 0.03459\n",
      "Epoch: 30, loss: 0.02424\n",
      "Epoch: 40, loss: 0.01293\n",
      "Epoch: 50, loss: 0.00917\n",
      "Epoch: 60, loss: 0.00753\n",
      "Epoch: 70, loss: 0.00649\n",
      "Epoch: 80, loss: 0.00583\n",
      "Epoch: 90, loss: 0.00531\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  outputs = lstm1.forward(X_train_tensors_final) #forward pass\n",
    "  optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    " \n",
    "  # obtain the loss function\n",
    "  loss = criterion(outputs, y_train_tensors)\n",
    " \n",
    "  loss.backward() #calculates the loss of the loss function\n",
    " \n",
    "  optimizer.step() #improve from loss, i.e backprop\n",
    "  if epoch % 10 == 0:\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0055, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(criterion(lstm1.forward(X_test_tensors_final), y_test_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
